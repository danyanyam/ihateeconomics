{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение ДЗ 3\n",
    "\n",
    "Данное дз копия https://github.com/yandexdataschool/Practical_DL/blob/fall19/week05_nlp/part2_pytorch.ipynb\n",
    "\n",
    "Однако содержит дополнительные задания на применение tf-idf и bag-of-words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Deep Learning\n",
    "\n",
    "Today we're gonna apply the newly learned DL tools for sequence processing to the task of predicting job salary.\n",
    "\n",
    "Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the assignment core (orignally written for theano/tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the challenge\n",
    "For starters, let's download the data from __[here](https://yadi.sk/d/vVEOWPFY3NruT7)__.\n",
    "\n",
    "You can also get it from the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (in that case, pick `Train_rev1.*`).\n",
    "\n",
    "\n",
    "Our task is to predict one number, __SalaryNormalized__, in the sense of minimizing __Mean Absolute Error__.\n",
    "\n",
    "<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "To do so, our model ca access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
    "\n",
    "\n",
    "You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66234</th>\n",
       "      <td>68712797</td>\n",
       "      <td>Profit Analyst</td>\n",
       "      <td>Responsible for the delivery of core analytica...</td>\n",
       "      <td>Basingstoke Hampshire South East</td>\n",
       "      <td>Basingstoke</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>COMPUTER 2000</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>From 23,000 to 28,000 per annum PLUS 25% Bonus...</td>\n",
       "      <td>25500</td>\n",
       "      <td>totaljobs.com</td>\n",
       "      <td>10.146473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168577</th>\n",
       "      <td>71333374</td>\n",
       "      <td>Clinical Training Low Intensity IAPT  Associat...</td>\n",
       "      <td>(Ref. P****) This fulltime post is available *...</td>\n",
       "      <td>South West England</td>\n",
       "      <td>South West England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>University of Exeter</td>\n",
       "      <td>Teaching Jobs</td>\n",
       "      <td>25,582 . pro rata</td>\n",
       "      <td>25582</td>\n",
       "      <td>Jobs Ac</td>\n",
       "      <td>10.149683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>66553103</td>\n",
       "      <td>RFP Exec</td>\n",
       "      <td>The role will involve completing RFP's / RFI's...</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mirage Recruitment</td>\n",
       "      <td>Accounting &amp; Finance Jobs</td>\n",
       "      <td>Circa 40k - 50k pro-rata</td>\n",
       "      <td>45000</td>\n",
       "      <td>eFinancialCareers</td>\n",
       "      <td>10.714440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "66234   68712797                                     Profit Analyst   \n",
       "168577  71333374  Clinical Training Low Intensity IAPT  Associat...   \n",
       "12455   66553103                                           RFP Exec   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "66234   Responsible for the delivery of core analytica...   \n",
       "168577  (Ref. P****) This fulltime post is available *...   \n",
       "12455   The role will involve completing RFP's / RFI's...   \n",
       "\n",
       "                             LocationRaw  LocationNormalized ContractType  \\\n",
       "66234   Basingstoke Hampshire South East         Basingstoke          NaN   \n",
       "168577                South West England  South West England          NaN   \n",
       "12455                             London              London          NaN   \n",
       "\n",
       "       ContractTime               Company                   Category  \\\n",
       "66234     permanent         COMPUTER 2000                    IT Jobs   \n",
       "168577          NaN  University of Exeter              Teaching Jobs   \n",
       "12455           NaN    Mirage Recruitment  Accounting & Finance Jobs   \n",
       "\n",
       "                                                SalaryRaw  SalaryNormalized  \\\n",
       "66234   From 23,000 to 28,000 per annum PLUS 25% Bonus...             25500   \n",
       "168577                                  25,582 . pro rata             25582   \n",
       "12455                            Circa 40k - 50k pro-rata             45000   \n",
       "\n",
       "               SourceName  Log1pSalary  \n",
       "66234       totaljobs.com    10.146473  \n",
       "168577            Jobs Ac    10.149683  \n",
       "12455   eFinancialCareers    10.714440  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"kaggle/dz/Train_rev1.csv\", index_col=None)\n",
    "\n",
    "# Добавляем логарифм зарплаты\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast nan to string\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP part\n",
    "\n",
    "To even begin training our neural network, we're gonna need to preprocess the text features: tokenize it and build the token vocabularies.\n",
    "\n",
    "Since it is not an NLP course, we're gonna use simple built-in NLTK tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "0                               Engineering Systems Analyst\n",
      "10000                            Optometrist Job Canterbury\n",
      "20000                                     Security officers\n",
      "30000                                           Night Nurse\n",
      "40000                                   App Developer (iOS)\n",
      "50000     Smart Metering Project Manager (Complex Networ...\n",
      "60000                                        Office Manager\n",
      "70000                             Internal Sales Negotiator\n",
      "80000                                     Contracts Manager\n",
      "90000                     Structural Design Engineer  Dubai\n",
      "100000                                         HR Assistant\n",
      "110000    Senior Solutions Architect  Bellshill, North L...\n",
      "120000                                          Lead Tester\n",
      "130000                           Treasury Assistant/Cashier\n",
      "140000           Deputy Manager Cool NEW GASTRO PUB Concept\n",
      "150000     Programmes Director  Global Defence Organisation\n",
      "160000         Assessor/Tutor  Retail and Business Services\n",
      "170000                                      General Manager\n",
      "180000                                            Developer\n",
      "190000                  Analytical Team Leader (Inhalation)\n",
      "200000                                 Senior EC&I Engineer\n",
      "210000                                     Junior Artworker\n",
      "220000                               Assistant Designer  UK\n",
      "230000                               Administration Officer\n",
      "240000    Customer Service Representative (Internet / Fi...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Before\")\n",
    "print(data[\"Title\"][::10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "# Мы берем каждый столбец с текстовыми признаками\n",
    "# И делаем так, чтобы между любыми символами, кроме объединений слов стоял пробел\n",
    "\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(lambda l: ' '.join(tokenizer.tokenize(str(l).lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After\n",
      "0                               engineering systems analyst\n",
      "10000                            optometrist job canterbury\n",
      "20000                                     security officers\n",
      "30000                                           night nurse\n",
      "40000                                 app developer ( ios )\n",
      "50000     smart metering project manager ( complex netwo...\n",
      "60000                                        office manager\n",
      "70000                             internal sales negotiator\n",
      "80000                                     contracts manager\n",
      "90000                      structural design engineer dubai\n",
      "100000                                         hr assistant\n",
      "110000    senior solutions architect bellshill , north l...\n",
      "120000                                          lead tester\n",
      "130000                         treasury assistant / cashier\n",
      "140000           deputy manager cool new gastro pub concept\n",
      "150000      programmes director global defence organisation\n",
      "160000        assessor / tutor retail and business services\n",
      "170000                                      general manager\n",
      "180000                                            developer\n",
      "190000                analytical team leader ( inhalation )\n",
      "200000                               senior ec & i engineer\n",
      "210000                                     junior artworker\n",
      "220000                                assistant designer uk\n",
      "230000                               administration officer\n",
      "240000    customer service representative ( internet / f...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"After\")\n",
    "print(data[\"Title\"][::10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's see how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "# Считает, сколько раз каждый токен появляется в \"Title\" и \"FullDescription\"\n",
    "# Для каждого текста из объединенного вектора всех текстов\n",
    "for text in np.hstack([data.FullDescription.values, data.Title.values]):\n",
    "    # Разбиваем предложение на токены\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Для каждого токена ведём счёт\n",
    "    for token in tokens:\n",
    "        token_counts[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 202704\n",
      "('and', 2657388)\n",
      "('.', 2523216)\n",
      "(',', 2318606)\n",
      "('the', 2080994)\n",
      "('to', 2019884)\n",
      "...\n",
      "('improvemen', 1)\n",
      "('techniciancivil', 1)\n",
      "('mlnlycke', 1)\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим, сколько слов встречаются от 0 до 10000 раз\n",
    "\n",
    "_ = plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных и TF-IDF (3 балла)\n",
    "\n",
    "В данном задании вам необходимо реализовать TF-IDF.\n",
    "\n",
    "Использование готовых реализаций запрещено."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.1 (1 балл)\n",
    "Возьмите топ 20% наиболее оплачиваемых вакансий и топ 20% наименее оплачиваемых вакансий.\n",
    "\n",
    "В каждом классе посчитайте частоту слов.\n",
    "\n",
    "Напечатайте топ 20 слов из каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_twenty = data.sort_values(by=['SalaryNormalized'], ascending = False).iloc[:int(0.2 * len(data)), :]\n",
    "least_twenty = data.sort_values(by=['SalaryNormalized'], ascending = True).iloc[:int(0.2 * len(data)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_c = Counter()\n",
    "least_twenty_c = Counter()\n",
    "\n",
    "# Считает, сколько раз каждый токен появляется в \"Title\"\n",
    "# Для каждого текста из вектора всех заголовков\n",
    "for text in top_twenty.Title.values:\n",
    "    # Разбиваем предложение на токены\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Для каждого токена ведём счёт\n",
    "    for token in tokens:\n",
    "        top_twenty_c[token] += 1 / len(tokens)\n",
    "\n",
    "# То же самое для второго класса\n",
    "for text in least_twenty.Title.values:\n",
    "    # Разбиваем предложение на токены\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Для каждого токена ведём счёт\n",
    "    for token in tokens:\n",
    "        least_twenty_c[token] += 1 / len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для первого класса\n",
      "[('manager', 3862.4939578152466),\n",
      " ('senior', 1551.5804894983141),\n",
      " ('engineer', 1250.0886998736833),\n",
      " ('/', 1076.7991668518014),\n",
      " ('developer', 943.636024457475),\n",
      " (',', 848.7848933742188),\n",
      " ('business', 758.0552357014112),\n",
      " ('project', 714.1654720694509),\n",
      " ('****', 706.2574350298762),\n",
      " ('analyst', 681.7984268784195),\n",
      " ('director', 582.9874490061017),\n",
      " ('consultant', 562.7691138926419),\n",
      " ('sales', 521.9977427800937),\n",
      " ('london', 470.3863391451932),\n",
      " ('of', 458.7794776201868),\n",
      " ('(', 452.51760879309757),\n",
      " ('financial', 442.8946886446861),\n",
      " ('finance', 439.7037979564265),\n",
      " (')', 438.556531383011),\n",
      " ('development', 436.58088659705993)]\n",
      "----------\n",
      "Для второго класса\n",
      "[('assistant', 2229.7359416726995),\n",
      " ('administrator', 1470.897836885334),\n",
      " ('sales', 1292.7848720723662),\n",
      " ('manager', 972.2551258382229),\n",
      " ('/', 960.6904585537674),\n",
      " ('support', 941.2078241203394),\n",
      " ('cleaner', 759.5398268398292),\n",
      " ('worker', 632.7267260517264),\n",
      " ('customer', 628.5180911479795),\n",
      " ('executive', 626.8632992008002),\n",
      " ('service', 600.3887792017578),\n",
      " ('care', 594.9827575202584),\n",
      " ('advisor', 557.073035298034),\n",
      " ('****', 542.8329385267099),\n",
      " ('chef', 492.22169080918616),\n",
      " ('consultant', 417.5334568209548),\n",
      " ('operative', 411.1262737262719),\n",
      " ('coordinator', 371.5846431346421),\n",
      " ('accounts', 357.5014985014975),\n",
      " ('receptionist', 350.72466838716764)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print('Для первого класса')\n",
    "pprint(top_twenty_c.most_common(20))\n",
    "print('-'*10)\n",
    "\n",
    "print('Для второго класса')\n",
    "pprint(least_twenty_c.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.2 (2 балла)\n",
    "\n",
    "Возьмите топ 20% наиболее оплачиваемых вакансий и топ 20% наименее оплачиваемых вакансий. \n",
    "\n",
    "В каждом классе посчитайте суммарный tf-idf для каждого слова\n",
    "\n",
    "Напечатайте топ 20 слов из каждого класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для 20% наиболее оплачиваемых вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7885/7885 [06:14<00:00, 21.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# IDF термина а = логарифм(Общее количество документов / Количество документов, в которых встречается термин а)\n",
    "import tqdm\n",
    "titles = top_twenty['Title']\n",
    "# Каждый отдельный заголовок - это новый документ\n",
    "all = len(titles)\n",
    "\n",
    "# Для каждого слова, которое встречалось в топ-20 заголовках\n",
    "for word in tqdm.tqdm(top_twenty_c):\n",
    "    # Обнуляем счетчик\n",
    "    counte = 0\n",
    "    # Для каждого заголовка из всех заголовков\n",
    "    for title in titles:\n",
    "        # Для каждого слова внутри одного заголовка\n",
    "        for wor in title.split(' '):\n",
    "            # Если слово внутри заголовка совпадает со словом, из топ 20\n",
    "            if word == wor:\n",
    "                # То увеличиваем счетчик на 1\n",
    "                counte +=1\n",
    "    # Пройдя все заголовки для одного слова, считаем idf и tf-idf\n",
    "    idf = np.log(all / counte)\n",
    "    # Перезаписываем в словарь подсчитанный tf-idf, вместо tf\n",
    "    top_twenty_c[word] = top_twenty_c[word] * idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('manager', 4692.091349689706),\n",
      " ('senior', 3047.05911613155),\n",
      " ('engineer', 2879.278594197559),\n",
      " ('developer', 2264.4961441832124),\n",
      " ('project', 2027.5090820536136),\n",
      " ('business', 2026.2702322287266),\n",
      " ('analyst', 1961.8362550471738),\n",
      " ('/', 1917.008202529264),\n",
      " ('director', 1800.243444401045),\n",
      " ('consultant', 1677.8359798405006),\n",
      " ('sales', 1574.6806794818146),\n",
      " ('financial', 1536.0521075483196),\n",
      " ('finance', 1528.747311241569),\n",
      " ('****', 1518.8560049259465),\n",
      " (',', 1503.9092323010982),\n",
      " ('of', 1423.1104737332369),\n",
      " ('development', 1392.3653285154926),\n",
      " ('head', 1338.8264863700872),\n",
      " ('london', 1281.6097826448363),\n",
      " ('accountant', 1258.9049230264504)]\n"
     ]
    }
   ],
   "source": [
    "pprint(top_twenty_c.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для 20% наименее оплачиваемых вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8072/8072 [05:37<00:00, 23.88it/s]\n"
     ]
    }
   ],
   "source": [
    "titles = least_twenty['Title']\n",
    "# Каждый отдельный заголовок - это новый документ\n",
    "all = len(titles)\n",
    "\n",
    "# Для каждого слова, которое встречалось в худших-20% заголовках\n",
    "for word in tqdm.tqdm(least_twenty_c):\n",
    "    # Обнуляем счетчик\n",
    "    counte = 0\n",
    "    # Для каждого заголовка из всех заголовков\n",
    "    for title in titles:\n",
    "        # Для каждого слова внутри одного заголовка\n",
    "        for wor in title.split(' '):\n",
    "            # Если слово внутри заголовка совпадает со словом, из худших-20%\n",
    "            if word == wor:\n",
    "                # То увеличиваем счетчик на 1\n",
    "                counte +=1\n",
    "    # Пройдя все заголовки для одного слова, считаем idf и tf-idf\n",
    "    idf = np.log(all / counte)\n",
    "    # Перезаписываем в словарь подсчитанный tf-idf, вместо tf\n",
    "    least_twenty_c[word] = least_twenty_c[word] * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('assistant', 4266.492335873799),\n",
      " ('administrator', 3905.378680538865),\n",
      " ('sales', 2955.609115358434),\n",
      " ('cleaner', 2894.000737801721),\n",
      " ('manager', 2613.605238452556),\n",
      " ('support', 2486.7705636080336),\n",
      " ('/', 2105.0807590111363),\n",
      " ('executive', 1979.0397139262006),\n",
      " ('worker', 1945.3887268186631),\n",
      " ('customer', 1863.0681777876719),\n",
      " ('care', 1803.098551545821),\n",
      " ('service', 1801.7150314645928),\n",
      " ('advisor', 1793.4676502433686),\n",
      " ('operative', 1596.7671842366285),\n",
      " ('chef', 1520.845085955498),\n",
      " ('receptionist', 1502.4705440358396),\n",
      " ('consultant', 1460.591788863895),\n",
      " ('coordinator', 1408.6830744866954),\n",
      " ('accounts', 1403.3550076510771),\n",
      " ('labourer', 1376.0684111367218)]\n"
     ]
    }
   ],
   "source": [
    "pprint(least_twenty_c.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words (3 балла)\n",
    "\n",
    "В данном задании вам необходимо построить регрессию поверх bag-of-words\n",
    "\n",
    "Можно использовать любые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель качества подгонки регрессии составил: 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Создаём bag of words из всего датасета, чтобы для каждого слова был ключ в словаре\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer = vectorizer.fit(data['Title'])\n",
    "\n",
    "# Разбиваем выборку на обучающую и отложенную\n",
    "train_, test_, train_labels, test_labels = train_test_split(data['Title'], data['Log1pSalary'])\n",
    "\n",
    "# Переводим разбитую подвыборку, представленную в виде текста в векторный формат\n",
    "train_ = vectorizer.transform(train_.values) \n",
    "test_ = vectorizer.transform(test_.values) \n",
    "\n",
    "# Строим регрессия по bag of words\n",
    "clf = linear_model.LinearRegression().fit(train_, train_labels)\n",
    "pred = clf.predict(test_)\n",
    "\n",
    "# Оцениваем построенную регрессию\n",
    "print('Показатель качества подгонки регрессии составил: {:3.2f}'.format(r2_score(test_labels, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейросетевой подход (4 балла)\n",
    "\n",
    "Вам необходимо сделать весь оставшийся ноутбук.\n",
    "\n",
    "\n",
    "enjoy :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.1__ Получите список всех токенов, которые встречаются хотя бы 10 раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# Выбираем те токены, которые встречались не менее определенного порога раз\n",
    "tokens = [token for token, count in token_counts.items() if count >= min_count] \n",
    "\n",
    "# Просто добавляем токены для неизвестных и пустых слов\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens left: 34158\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens left:\", len(tokens))\n",
    "assert type(tokens)==list\n",
    "assert len(tokens) in range(32000,35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ Постройте обратный словарь (из индексов в токены)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {token: index for index, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем словарь, который мы только что построили, чтобы представить данные в удобный для торча формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В переменные помещаем индексы соответствующих токенов\n",
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    \"\"\" Конвертируем список токенов в матрицу с отступом \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        # Разбиваем каждое предложение на слова и создаем список слов из строк\n",
    "        sequences = list(map(str.split, sequences))\n",
    "    # Определяем длину строки. Она либо максимум из количества слов во всех строках, либо установленное число\n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    # Создаем матрицу, размера (количество ппредложений Х максимальную длину предложения) и заполняем её пустыми словами\n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    # Для номера, предложения в предложениях:\n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Новая строка - индекс слова, если оно новое, то UNKNOWN для всех слов из строки\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        # Заменяем в матрице строку нашей\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering systems analyst\n",
      "hr assistant\n",
      "senior ec & i engineer\n",
      "\n",
      "Matrix:\n",
      "[[    2     3     4     1     1]\n",
      " [  548  2361     1     1     1]\n",
      " [  537 10662   390   307    32]]\n"
     ]
    }
   ],
   "source": [
    "#### print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте закодируем наши категориальные признаки.\n",
    "\n",
    "Как обычно для простоты мы будем использовать one-hot кодирование. Можно применять другие техники: (tf-idf, target averaging or pseudo-counter-based encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, separator='=', sort=True,\n",
       "               sparse=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Рассмотрим только топ 1000 самых частых компаний, чтобы сэкономить потребление памяти\n",
    "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data science part\n",
    "Как только мы научились токенезировать данные, пора устраивать эксперимент.\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split.\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  220291\n",
      "Validation size =  24477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Создает совместимый с пайторчем словарь из батча данных\n",
    "    Возвращает: словарь с {'title' : int64[batch, title_max_len]}\n",
    "    \n",
    "    \"\"\"\n",
    "    # Если указан размет батча\n",
    "    if batch_size is not None:\n",
    "        # То из данных берем подвыборку размерностью батча\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "    \n",
    "    batch = {}\n",
    "    # Для каждого столбца, содержащего текст\n",
    "    for col in text_columns:\n",
    "        # Засовываем в словарь матричное представление всего столбца данных\n",
    "        batch[col] = as_matrix(data[col].values, max_len)\n",
    "    # Категориальный вектор также засовываем под отдельный ключ\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': array([[ 2980,  7005,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1],\n",
       "        [ 3714,  7896,  2712,   130,  1398,     1,     1,     1,     1,\n",
       "             1],\n",
       "        [ 3906,    58,  1362,    89,   417,   270,   130, 17226,   130,\n",
       "          1340]]),\n",
       " 'FullDescription': array([[  49,  171,   46, 2980, 7005,  130, 4503,  103,  533,  119],\n",
       "        [  10,   11,   15,  183,  458,    2,  186,   14, 2529,  309],\n",
       "        [3906,   58, 1098,   89,  417, 3906,  867,   22,   89,   21]]),\n",
       " 'Categorical': array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " 'Log1pSalary': array([ 9.351927,  9.639587, 10.434145], dtype=float32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(data_train, 3, max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Приступаем к нейронным сетям\n",
    "\n",
    "Наша модель состоит из 3-х веток\n",
    "* Энкодер заголовка\n",
    "* Энкодер описания\n",
    "* Энкодер категориальных признаков\n",
    "\n",
    "После создания этих веток, мы скормим нейронке эти данные, чтобы предсказывать зарплату\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/Practical_DL/raw/master/homework04/conv_salary_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По стандарту, оба текстовых векторайзера используют 1D convolutions, за которыми следует global pooling.\n",
    "\n",
    "\n",
    "By default, both text vectorizers shall use 1d convolutions, followed by global pooling over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "# Сделаем так, чтобы работала быстро моя нейроночка\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        \"\"\" \n",
    "        Простой энкодер последовательности для заголовков\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        # Сначала создаем слой, который каждому вектору из bag of words сопоставит вектор\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.conv1 = nn.Conv1d(64, out_size, kernel_size=3, padding=1)\n",
    "        self.pool1 = GlobalMaxPooling()\n",
    "        self.bn1 = nn.BatchNorm1d(out_size)\n",
    "        self.dense = nn.Linear(out_size, out_size)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: батч размера   [batch_size, max_len]\n",
    "        :returns: батч другого размера [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        h = self.emb(text_ix)\n",
    "        # Меняем порядок размерности [batch, time, units] на [batch, units, time] чтобы соответствовать порядку Conv1d\n",
    "        h = torch.transpose(h, 1, 2)\n",
    "        # Apply the layers as defined above. Add some ReLUs before dense.\n",
    "        #<YOUR CODE>\n",
    "        h = self.conv1(h)\n",
    "        h = F.relu(self.pool1(h))\n",
    "        h = F.relu(self.bn1(h))\n",
    "        h = self.dense(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "title_encoder = TitleEncoder(out_size=64).to(device)\n",
    "\n",
    "# Мы взяли три случайных заголовка, сгенерировали из них батчи по принципу bag of words\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['Title'])).to(device)\n",
    "\n",
    "# А уже эти батчи мы проводили полностью через энкодер\n",
    "dummy_v = title_encoder(dummy_x)\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ Create description encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Придумайте энкодер для описания работы\n",
    "    \n",
    "class DescEncoder(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(n_tokens, 64, padding_idx=PAD_IX)\n",
    "        self.rnn = nn.LSTM(input_size=64, hidden_size=90, num_layers=1, bidirectional=True, batch_first=True) \n",
    "        self.pool = GlobalMaxPooling(dim=1)\n",
    "        self.dense = nn.Linear(200, out_size)\n",
    "\n",
    "    def forward(self, text_ix, hidden=None):\n",
    "        h = self.emb(text_ix)\n",
    "        output, hidden = self.rnn(h, hidden)\n",
    "        h = self.pool(output)\n",
    "        h = self.dense(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine too\n"
     ]
    }
   ],
   "source": [
    "desc_encoder = DescEncoder(out_size=64).to(device)\n",
    "\n",
    "dummy_x = Variable(torch.LongTensor(generate_batch(data_train, 3)['FullDescription']))\n",
    "dummy_v = desc_encoder(dummy_x.to(device))\n",
    "\n",
    "assert isinstance(dummy_v, Variable)\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 64)\n",
    "del desc_encoder\n",
    "print(\"Seems fine too\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__ Постройте нейросеть ~~чтобы управлять ими~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This class does all the steps from (title, desc, categorical) features -> predicted target\n",
    "    It unites title & desc encoders you defined above as long as some layers for head and categorical branch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_)):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.title_encoder = TitleEncoder(out_size=64)\n",
    "        self.desc_encoder = DescEncoder(out_size=64)\n",
    "        \n",
    "        # Придумай слои для категорильных признаков. Несколько полносвязных сойдет.\n",
    "        # <YOUR CODE>\n",
    "        self.dense1 = nn.Linear(n_cat_features, 128)\n",
    "        self.dense2 = nn.Linear(128, 64)\n",
    "        self.dense3 = nn.Linear(64, 32)\n",
    "        \n",
    "        # Придумай слой \"выхода\", который обработает все остальные модули нейросети\n",
    "        #<YOUR CODE>\n",
    "        self.output = nn.Linear(160, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, title_ix, desc_ix, cat_features):\n",
    "        \"\"\"\n",
    "        :param title_ix: int32 Variable [batch, title_len], job titles encoded by as_matrix\n",
    "        :param desc_ix:  int32 Variable [batch, desc_len] , job descriptions encoded by as_matrix\n",
    "        :param cat_features: float32 Variable [batch, n_cat_features]\n",
    "        :returns: float32 Variable 1d [batch], predicted log1p-salary\n",
    "        \"\"\"\n",
    "        \n",
    "        # Проведи каждую дату по соответсвующим энкодерам\n",
    "        title_h = self.title_encoder(title_ix)\n",
    "        desc_h = self.desc_encoder(desc_ix)\n",
    "        \n",
    "        # Примени категориальный энкодер\n",
    "        cat_h = F.relu(self.dense1(cat_features))\n",
    "        cat_h = F.relu(self.dense2(cat_h))\n",
    "        cat_h = self.dense3(cat_h)\n",
    "        \n",
    "        \n",
    "        # Соедини все матрицы вместе\n",
    "        joint_h = torch.cat([title_h, desc_h, cat_h], dim=1)\n",
    "        \n",
    "        # ... и добавь парочку слоев еще\n",
    "        output = self.output(joint_h)\n",
    "        \n",
    "        # Note 1: do not forget to select first columns, [:, 0], to get to 1d outputs\n",
    "        # Note 2: please do not use output nonlinearities.\n",
    "        \n",
    "        # Просто хотим избавиться от лишнего измерения\n",
    "        return output[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullNetwork().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем на батче\n",
    "batch = generate_batch(data_train, 32)\n",
    "\n",
    "title_ix = Variable(torch.LongTensor(batch[\"Title\"])).to(device)\n",
    "desc_ix = Variable(torch.LongTensor(batch[\"FullDescription\"])).to(device)\n",
    "cat_features = Variable(torch.FloatTensor(batch[\"Categorical\"])).to(device)\n",
    "reference = Variable(torch.FloatTensor(batch[target_column])).to(device)\n",
    "\n",
    "# Предсказания для батча (поэтому вектор из 32 штучек)\n",
    "prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "assert len(prediction.shape) == 1 and prediction.shape[0] == title_ix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(reference, prediction):\n",
    "    \"\"\"\n",
    "    Computes objective for minimization.\n",
    "    By deafult we minimize MSE, but you are encouraged to try mix up MSE, MAE, huber loss, etc.\n",
    "    \"\"\"\n",
    "    return torch.mean((prediction - reference) ** 2)\n",
    "\n",
    "def compute_mae(reference, prediction):\n",
    "    \"\"\" Compute MAE on actual salary, assuming your model outputs log1p(salary)\"\"\"\n",
    "    return torch.abs(torch.exp(reference - 1) - torch.exp(prediction - 1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(reference, prediction)\n",
    "dummy_grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n",
    "for grad in dummy_grads:\n",
    "    assert grad is not None and not (grad == 0).all(), \"Some model parameters received zero grads. \" \\\n",
    "                                                       \"Double-check that your model uses all it's layers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "max_len = 100\n",
    "batch_size = 64\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\venvs\\new_project\\env\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64d544114394a65bb20a09a4f4ba989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10393\n",
      "\tMAE:\t3141.76187\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1194335f038d4b189d465baef5fac3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09100\n",
      "\tMAE:\t2859.38650\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c8ad8fecb54f51a38d0b157c4c92ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09865\n",
      "\tMAE:\t3002.18761\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af6573fd81d426fb148333d295f5d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08952\n",
      "\tMAE:\t2842.63298\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23a402d26ac433b8c6943d8abce82b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10218\n",
      "\tMAE:\t3093.40226\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e075fd37316b46ffbc6be23426bdf194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09214\n",
      "\tMAE:\t2946.02167\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e78a8df4b843e680eb4f53447ee5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09330\n",
      "\tMAE:\t2870.86170\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de7f2993e4d42e0a67b1bdbd428d4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08779\n",
      "\tMAE:\t2823.89131\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f571b8882c07418c903e106dbcb8ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09755\n",
      "\tMAE:\t2979.42778\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3236e9baa79b4355a88d92007ff49965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09350\n",
      "\tMAE:\t2976.75328\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c3b6d7740243c18d13f63508df3c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09804\n",
      "\tMAE:\t3061.73939\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ebc6801c0e471898329d08eaaaf93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08548\n",
      "\tMAE:\t2781.91590\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff16e6eebce46f2b7b7bc343e2bfb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10164\n",
      "\tMAE:\t3111.21841\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6332f334134834a3f3b238da3d5187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08651\n",
      "\tMAE:\t2790.35756\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e41f0efee814675a317b37c91ea8217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09878\n",
      "\tMAE:\t3005.97292\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d2bf71136b459db3bd7716dda184d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10171\n",
      "\tMAE:\t3020.85235\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de928ffcd7f446b89bcf198cef09b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09060\n",
      "\tMAE:\t2874.40453\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd090e4d2f14923ae4f394fa2315288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.13521\n",
      "\tMAE:\t3482.00354\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b082bcd1b8b4d5d8015f245673386d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09755\n",
      "\tMAE:\t2987.95694\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f2c74c221646c59c8d4316aa9f711b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08645\n",
      "\tMAE:\t2785.25128\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbfefc979044eaa82b0e022b933c456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08946\n",
      "\tMAE:\t2838.89897\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f04a35fa884402a6a4d0804a643b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08543\n",
      "\tMAE:\t2767.11811\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c6be1b1f034e519adb56851b3de1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09401\n",
      "\tMAE:\t2889.32100\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f75617ca9b44df2b0344dc8c7216caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10064\n",
      "\tMAE:\t2984.89072\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e57434675b44c7975f7a124d8f4baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09104\n",
      "\tMAE:\t2896.68809\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2049ee871f4542dd947c6df5c2f49776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08571\n",
      "\tMAE:\t2806.91695\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cbdee2150a42468b4ad31f23fa0ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08922\n",
      "\tMAE:\t2887.73614\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee5895fa346416c9dabba440f095ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08431\n",
      "\tMAE:\t2821.35125\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbed9e64c6f4a6aada512e5cdfdf091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08864\n",
      "\tMAE:\t2882.40104\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a54d9cf70041dda96462c69e305f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08435\n",
      "\tMAE:\t2800.38023\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d2ff9abb347eda536f915773d15e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09334\n",
      "\tMAE:\t2964.35309\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be203ba4e31412aa8dd18608811e859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10445\n",
      "\tMAE:\t3264.38879\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bbfe0a12164300bac3dc924ff0c46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.10200\n",
      "\tMAE:\t3101.24911\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004bb0652aee415f9623e04a6973f37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08504\n",
      "\tMAE:\t2766.03915\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454af9b4a9694b468ff104c6600efa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.09668\n",
      "\tMAE:\t2903.89885\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398086b825514d06a78c26e2a81a9c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08149\n",
      "\tMAE:\t2696.04259\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7377e782e11f41959abca87aef0607a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08751\n",
      "\tMAE:\t2900.14717\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d226a47b389f46759a502a68382dc2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08111\n",
      "\tMAE:\t2714.63821\n",
      "\n",
      "\n",
      "\n",
      "Training:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e1f9a4bfe54a5cbf58c09e3bab3807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08729\n",
      "\tMAE:\t2835.90796\n",
      "\n",
      "\n",
      "\n",
      "Validation:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cf8d6456d0414b9ee52385cb05039f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08228\n",
      "\tMAE:\t2703.82597\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(num_epochs):\n",
    "    \n",
    "    print(\"Training:\")\n",
    "    train_loss = train_mae = train_batches = 0    \n",
    "    model.train(True)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch):\n",
    "\n",
    "        title_ix = Variable(torch.LongTensor(batch[\"Title\"])).to(device)\n",
    "        desc_ix = Variable(torch.LongTensor(batch[\"FullDescription\"])).to(device)\n",
    "        cat_features = Variable(torch.FloatTensor(batch[\"Categorical\"])).to(device)\n",
    "        reference = Variable(torch.FloatTensor(batch[target_column])).to(device)\n",
    "\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "\n",
    "        loss = compute_loss(reference, prediction)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss += loss.data.cpu().numpy()\n",
    "        train_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        train_batches += 1\n",
    "    \n",
    "    print(\"\\tLoss:\\t%.5f\" % (train_loss / train_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (train_mae / train_batches))\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    print(\"Validation:\")\n",
    "    val_loss = val_mae = val_batches = 0\n",
    "    model.train(False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "            title_ix = Variable(torch.LongTensor(batch[\"Title\"])).to(device)\n",
    "            desc_ix = Variable(torch.LongTensor(batch[\"FullDescription\"])).to(device)\n",
    "            cat_features = Variable(torch.FloatTensor(batch[\"Categorical\"])).to(device)\n",
    "            reference = Variable(torch.FloatTensor(batch[target_column])).to(device)\n",
    "\n",
    "            prediction = model(title_ix, desc_ix, cat_features)\n",
    "            loss = compute_loss(reference, prediction)\n",
    "\n",
    "            val_loss += loss.data.cpu().numpy()\n",
    "            val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "            val_batches += 1\n",
    "\n",
    "    print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "    print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30 epochs MAE < 2800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\venvs\\new_project\\env\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ce47efe6cc4638850e53013a7cd8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=765.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tLoss:\t0.08228\n",
      "\tMAE:\t2703.82597\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final eval:\")\n",
    "val_loss = val_mae = val_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in iterate_minibatches(data_val, shuffle=False):\n",
    "        title_ix = Variable(torch.LongTensor(batch[\"Title\"])).to(device)\n",
    "        desc_ix = Variable(torch.LongTensor(batch[\"FullDescription\"])).to(device)\n",
    "        cat_features = Variable(torch.FloatTensor(batch[\"Categorical\"])).to(device)\n",
    "        reference = Variable(torch.FloatTensor(batch[target_column])).to(device)\n",
    "\n",
    "        prediction = model(title_ix, desc_ix, cat_features)\n",
    "        loss = compute_loss(reference, prediction)\n",
    "\n",
    "        val_loss += loss.data.cpu().numpy()\n",
    "        val_mae += compute_mae(reference, prediction).data.cpu().numpy()\n",
    "        val_batches += 1\n",
    "\n",
    "print(\"\\tLoss:\\t%.5f\" % (val_loss / val_batches))\n",
    "print(\"\\tMAE:\\t%.5f\" % (val_mae / val_batches))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонусная секция. Любые разумные попытки улучшить архитектуру поощряются дополнительными баллами. Чтобы мы не пропустили ваши старания - опишите ниже, что вы сделали (это должно быть видно и в коде). (max 4 балла).\n",
    "\n",
    "\n",
    "### Task 3.2: Actually make it work\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__.\n",
    "\n",
    "Try __at least 3 options__ from the list below for a passing grade. If you're into \n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm1d`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to do max pooling:\n",
    "* Max over time - our `GlobalMaxPooling`\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a small neural network\n",
    "\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$\n",
    "\n",
    "#### C) Fun with embeddings\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained word2vec from [here](http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/) or [here](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/).\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "  * Please bear in mind that while convolution uses [batch, units, time] dim order, \n",
    "    recurrent units are built for [batch, time, unit]. You may need to `torch.transpose`.\n",
    "\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [keras](https://github.com/keras-team/keras/blob/master/keras/callbacks.py#L461) for inspiration.\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.state_dict`\n",
    "  * Plotting learning curves is usually a good idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мои приколюхи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) Я добавил LSTM в DescriptionEncoder\n",
    "    2) Добавил Батч нормализацию"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
