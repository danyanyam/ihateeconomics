{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('D:/123.csv', header=None)\n",
    "columnsNames = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'Pred']\n",
    "data.columns = columnsNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = data.iloc[:, 0]\n",
    "age = np.array(age)\n",
    "age = age / 100\n",
    "data.iloc[:, 0] = age.astype(float)\n",
    "\n",
    "cp = data.iloc[:, 2]\n",
    "cp = np.array(cp)\n",
    "cp = cp / 4\n",
    "data.iloc[:, 2] = cp.astype(float)\n",
    "\n",
    "trestbps = data.iloc[:, 3]\n",
    "trestbps = np.array(trestbps)\n",
    "trestbps = trestbps / 200\n",
    "data.iloc[:, 3] = trestbps.astype(float)\n",
    "\n",
    "chol = data.iloc[:, 4]\n",
    "chol = np.array(chol)\n",
    "chol = chol / 564\n",
    "data.iloc[:, 4] = chol.astype(float)\n",
    "\n",
    "restecg = data.iloc[:, 6]\n",
    "restecg = np.array(restecg)\n",
    "restecg = restecg / 2\n",
    "data.iloc[:, 6] = restecg.astype(float)\n",
    "\n",
    "thalach = data.iloc[:, 7]\n",
    "thalach = np.array(thalach)\n",
    "thalach = thalach / 202\n",
    "data.iloc[:, 7] = thalach.astype(float)\n",
    "\n",
    "oldpeak = data.iloc[:, 9]\n",
    "oldpeak = np.array(oldpeak)\n",
    "oldpeak = oldpeak / 6.2\n",
    "data.iloc[:, 9] = oldpeak.astype(float)\n",
    "\n",
    "slope = data.iloc[:, 10]\n",
    "slope = np.array(slope)\n",
    "slope = slope / 3\n",
    "data.iloc[:, 10] = slope.astype(float)\n",
    "\n",
    "ca = data.iloc[:, 11]\n",
    "ca = np.array(ca)\n",
    "ca = ca / 3\n",
    "data.iloc[:, 11] = ca.astype(float)\n",
    "\n",
    "thal = data.iloc[:, 12]\n",
    "thal = np.array(thal)\n",
    "thal = thal / 7\n",
    "data.iloc[:, 12] = thal.astype(float)\n",
    "\n",
    "y_pred = data.iloc[:, -1]\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = y_pred / 4\n",
    "data.iloc[:, -1] = y_pred.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.545421</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.789562</td>\n",
       "      <td>0.658468</td>\n",
       "      <td>0.438564</td>\n",
       "      <td>0.144781</td>\n",
       "      <td>0.498316</td>\n",
       "      <td>0.740591</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.170251</td>\n",
       "      <td>0.534231</td>\n",
       "      <td>0.225589</td>\n",
       "      <td>0.675806</td>\n",
       "      <td>0.236532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090497</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.241215</td>\n",
       "      <td>0.088814</td>\n",
       "      <td>0.092194</td>\n",
       "      <td>0.352474</td>\n",
       "      <td>0.497457</td>\n",
       "      <td>0.113572</td>\n",
       "      <td>0.469761</td>\n",
       "      <td>0.188084</td>\n",
       "      <td>0.206062</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>0.276947</td>\n",
       "      <td>0.308638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.374113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.430851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.757426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.610000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean     0.545421    0.676768    0.789562    0.658468    0.438564    0.144781   \n",
       "std      0.090497    0.468500    0.241215    0.088814    0.092194    0.352474   \n",
       "min      0.290000    0.000000    0.250000    0.470000    0.223404    0.000000   \n",
       "25%      0.480000    0.000000    0.750000    0.600000    0.374113    0.000000   \n",
       "50%      0.560000    1.000000    0.750000    0.650000    0.430851    0.000000   \n",
       "75%      0.610000    1.000000    1.000000    0.700000    0.489362    0.000000   \n",
       "max      0.770000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean     0.498316    0.740591    0.326599    0.170251    0.534231    0.225589   \n",
       "std      0.497457    0.113572    0.469761    0.188084    0.206062    0.312988   \n",
       "min      0.000000    0.351485    0.000000    0.000000    0.333333    0.000000   \n",
       "25%      0.000000    0.658416    0.000000    0.000000    0.333333    0.000000   \n",
       "50%      0.500000    0.757426    0.000000    0.129032    0.666667    0.000000   \n",
       "75%      1.000000    0.821782    1.000000    0.258065    0.666667    0.333333   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "             thal        Pred  \n",
       "count  297.000000  297.000000  \n",
       "mean     0.675806    0.236532  \n",
       "std      0.276947    0.308638  \n",
       "min      0.428571    0.000000  \n",
       "25%      0.428571    0.000000  \n",
       "50%      0.428571    0.000000  \n",
       "75%      1.000000    0.500000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = data.iloc[:, :]\n",
    "matrix = np.array(matrix)\n",
    "np.random.shuffle(matrix)\n",
    "x_data = matrix[:, :13]\n",
    "y_data = matrix[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 13)\n",
      "(297,)\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Архитектура нейронной сети\n",
    "Всего 4 слоя:\n",
    "1. Входной\n",
    "    - 13 нейронов\n",
    "2. 2 скрытых \n",
    "    - 15 нейронов каждый\n",
    "3. 1 выходной\n",
    "    - 5 выходов\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y_data):\n",
    "    list = []\n",
    "    for i in range((y_data).shape[0]):\n",
    "        if y_data[i] == 0:\n",
    "            list.append([1, 0, 0, 0, 0,])\n",
    "        if y_data[i] == 0.25:\n",
    "            list.append([0, 1, 0, 0, 0,])\n",
    "        if y_data[i] == 0.5:\n",
    "            list.append([0, 0, 1, 0, 0,])\n",
    "        if y_data[i] == 0.75:\n",
    "            list.append([0, 0, 0, 1, 0,])\n",
    "        if y_data[i] == 1:\n",
    "            list.append([0, 0, 0, 0, 1,])\n",
    "    return(np.array(list))\n",
    "\n",
    "y_encoded = one_hot_encoding(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_data[:207,:]\n",
    "y_train = y_encoded[:207, :]\n",
    "x_test = x_data[207:, :]\n",
    "y_test = y_encoded[207:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing random weights\n",
    "\n",
    "n_inputs = 13\n",
    "l2_inputs = 15\n",
    "l3_inputs = 15\n",
    "l4_inputs = 5\n",
    "\n",
    "weights_1 = np.random.random((l2_inputs, n_inputs + 1))\n",
    "weights_2 = np.random.random((l3_inputs, l2_inputs + 1))\n",
    "weights_3 = np.random.random((l4_inputs, l3_inputs + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.056312773963697\n",
      "6.906246264168403\n",
      "5.590442885945343\n",
      "4.767028919684933\n",
      "3.4292227829427517\n",
      "2.962662796478136\n",
      "2.154869496396019\n",
      "2.4477502784217724\n",
      "2.3263680425952815\n",
      "2.0303534586039063\n",
      "1.7293075708228955\n",
      "1.4948939638158532\n",
      "1.3817096048037536\n",
      "1.9798853651956154\n",
      "1.89266142367979\n",
      "1.5807358360790071\n",
      "1.6197495170056553\n",
      "1.668447563940397\n",
      "1.4689818679977178\n",
      "1.2602848637426132\n",
      "1.8896881084537753\n",
      "1.2595923854464348\n",
      "1.1378532578486267\n",
      "1.0731931421946732\n",
      "1.782968355482054\n",
      "1.6097325457833158\n",
      "1.2419567993021672\n",
      "1.0704782875842678\n",
      "1.833944609095739\n",
      "1.376495890870439\n",
      "1.2136515273872237\n",
      "1.014872638936785\n",
      "1.3924238149218162\n",
      "1.5412242297744065\n",
      "1.1380345275927684\n",
      "0.948063518882704\n",
      "1.3846627135145417\n",
      "1.0019919838408013\n",
      "0.8615743086012857\n",
      "1.6066349240526492\n",
      "1.1517538737727584\n",
      "1.3771755116888045\n",
      "1.081966036192443\n",
      "1.2973464997771416\n",
      "0.9719097829467739\n",
      "1.241641854094977\n",
      "0.929093636058095\n",
      "0.766908944547066\n",
      "0.6925840469391157\n",
      "0.6488779362800097\n",
      "0.6181416675860124\n",
      "1.4062917311674998\n",
      "0.6973618908669043\n",
      "1.3951560112458865\n",
      "0.9626969542935266\n",
      "1.3979532601891287\n",
      "0.8885299327965714\n",
      "0.6898188215800054\n",
      "0.5962928838077708\n",
      "0.5507561500106933\n",
      "0.5191679953569904\n",
      "1.378664217861132\n",
      "0.5868852231638575\n",
      "1.3439253896954628\n",
      "0.9198535898905058\n",
      "0.7595342552467965\n",
      "0.5892594473085436\n",
      "0.9525788837798462\n",
      "1.1832037328942406\n",
      "0.7205102787708642\n",
      "0.5607607308400802\n",
      "0.48850824883491095\n",
      "1.0514423081010635\n",
      "0.7008353759336056\n",
      "0.7362075896387594\n",
      "0.6927802716089265\n",
      "0.7098431365590616\n",
      "0.5265292707027635\n",
      "0.44417933652914954\n",
      "0.40247215588732577\n",
      "0.3786970879389233\n",
      "1.2701672906068628\n",
      "0.4337656592436332\n",
      "0.39026418375049093\n",
      "1.0831509350072237\n",
      "0.4612879907483065\n",
      "0.3978836044643424\n",
      "0.9227426877144789\n",
      "0.483778455026869\n",
      "1.1982644811762402\n",
      "0.7033282172131095\n",
      "0.9659458329676647\n",
      "0.5751829484731938\n",
      "1.1249197981816415\n",
      "0.7322738573232687\n",
      "0.8025179269161076\n",
      "0.6198915961128146\n",
      "0.958862153508975\n",
      "0.5443760152345907\n",
      "0.428268747499056\n",
      "1.03144315585843\n",
      "0.8527714045612741\n",
      "0.6077658716276761\n",
      "0.6475131810693192\n",
      "0.4738084209280093\n",
      "0.38711637556843204\n",
      "0.9674282252112243\n",
      "0.6660210588912016\n",
      "0.5408228046603387\n",
      "0.8313357672359769\n",
      "0.49933379414306767\n",
      "0.9788971676571396\n",
      "0.6710182376387275\n",
      "0.9275727494659997\n",
      "0.5906045346042628\n",
      "0.44988554204462455\n",
      "0.36750146757351365\n",
      "0.3248821035035053\n",
      "0.9781510354899049\n",
      "0.8892663324022125\n",
      "0.4422288196140576\n",
      "0.8824177251914074\n",
      "0.7424150480424666\n",
      "0.5131750090127648\n",
      "0.7825729123590065\n",
      "0.4775985031712232\n",
      "0.383400683084235\n",
      "0.939037249698449\n",
      "0.39268892645511577\n",
      "0.33188744753824845\n",
      "0.8211819734979583\n",
      "0.37579736144244236\n",
      "0.6737516570507442\n",
      "0.419348695641583\n",
      "0.5919614628028411\n",
      "0.4494117821538267\n",
      "0.9314699670482103\n",
      "0.8455228250795263\n",
      "0.5644442947305768\n",
      "0.5369804196066662\n",
      "0.4200621258894581\n",
      "0.9486682689812176\n",
      "0.5996320161881351\n",
      "0.48112099214711757\n",
      "0.37959291539726714\n",
      "0.3231840673732182\n",
      "0.2907128032759495\n",
      "0.26498192403206355\n",
      "0.9581451940061162\n",
      "0.30087114385094715\n",
      "0.9321364489207685\n",
      "0.3227438373906213\n",
      "0.7879620536814904\n",
      "0.35440831741470025\n",
      "0.670657642113714\n",
      "0.3891195351096982\n",
      "0.3277747395971707\n",
      "0.28867177759418605\n",
      "0.2602675189082482\n",
      "0.9320528464087643\n",
      "0.29203908755428576\n",
      "0.9195957311727333\n",
      "0.31317271430964805\n",
      "0.2740237856324552\n",
      "0.25264922830641945\n",
      "0.23394041898629683\n",
      "0.22498762750314316\n",
      "0.924244712413194\n",
      "0.26235359465626595\n",
      "0.2454991704392796\n",
      "0.22828638599799006\n",
      "0.2189081620716208\n",
      "0.8857450092199001\n",
      "0.9138174074170166\n",
      "0.6246468779303709\n",
      "0.4842244661909272\n",
      "0.45494234439525566\n",
      "0.46833218805791615\n",
      "0.3766061343490394\n",
      "0.5295193181577145\n",
      "0.4158154571663238\n",
      "0.9460589109513761\n",
      "0.48149120425173936\n",
      "0.4488618805710659\n",
      "0.36394152000720104\n",
      "0.8544657334293133\n",
      "0.5375915049191285\n",
      "0.42001596947326614\n",
      "0.3471331767081418\n",
      "0.5737560656802034\n",
      "0.7660656285043016\n",
      "0.41133475636932765\n",
      "0.33947806813059456\n",
      "0.2919679448761323\n",
      "0.2609375121614819\n",
      "0.23544664732713244\n",
      "0.827372930886372\n",
      "0.9286066518305042\n",
      "0.31535096072574514\n",
      "0.6656576774884274\n",
      "0.5135128186959914\n",
      "0.41186445373609815\n",
      "0.7671985924475623\n",
      "0.3812701930911534\n",
      "0.6446502946810105\n",
      "0.5041738374574428\n",
      "0.44652471916195013\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "epoch = 1\n",
    "alpha = 0.1\n",
    "reg_const = 0.1\n",
    "\n",
    "# Adds stroke of ones\n",
    "def matrix_design(x):\n",
    "    '''\n",
    "    1. Variables:\n",
    "        x - (n,) array\n",
    "    2. Process:\n",
    "        Adds one to the entered vector\n",
    "    3. return\n",
    "        (n+1, 1) array    \n",
    "    '''\n",
    "    x = x.reshape((x.shape[0], 1))\n",
    "    ones_vect = np.ones(x.shape[1])\n",
    "    return np.vstack((ones_vect, x))\n",
    "\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(x, deriv=False):\n",
    "    '''\n",
    "    1. Variables:\n",
    "        x - (n+1, 1) array\n",
    "    2. Process:\n",
    "        returns activation function, or derivative of activation function\n",
    "    3. return:\n",
    "        (n+1, 1) array\n",
    "    '''\n",
    "    if deriv == False:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    if deriv == True:\n",
    "        return 1 / (1 + np.exp(-x)) * (1 - (1 / (1 + np.exp(-x))))\n",
    "\n",
    "    \n",
    "# Forward Propagation, on each layer new 1 is added in the top of the vector\n",
    "def forward_prop(X, weights_1, weights_2, weights_3):\n",
    "    '''\n",
    "    1. Variables:\n",
    "        X - (n+1, 1) array\n",
    "        weights_1 - (15, 14) matrix from 1st to 2nd layer\n",
    "        weights_2 - (15, 16) matrix of weights from 2nd to 3rd layer\n",
    "        weights_3 - (5, 16) matrix of weigths from 3rd to 4th layer\n",
    "    2. Process:\n",
    "        performs forward propagation \n",
    "    3. return:\n",
    "        a1 - (14, 1) - vector of inputs\n",
    "        a2 - (16, 1) - 2nd layer of nods\n",
    "        a3 - (16, 1) - 3rd layer of nods\n",
    "        a4 - (5, 1) - 4th layer of nods, also prediction\n",
    "    \n",
    "    '''\n",
    "    a1 = matrix_design(X)\n",
    "    a2 = matrix_design(sigmoid(weights_1 @ a1))\n",
    "    a3 = matrix_design(sigmoid(weights_2 @ a2))\n",
    "    a4 = sigmoid(weights_3 @ a3)\n",
    "    return a1, a2, a3, a4\n",
    "\n",
    "# Error Calculation\n",
    "'''\n",
    "НУЖНО ПОФИКСИТЬ!!!!\n",
    "'''\n",
    "def cross_entropy(y, y_pr):\n",
    "    '''\n",
    "    1. Variables:\n",
    "        y_labeled - (5, 1) array\n",
    "        y_prediction - (5, 1) array\n",
    "    2. Process:\n",
    "        calculates cross entropy loss, including L2 Regularization (no bias regularization)\n",
    "    3. return:\n",
    "        Float\n",
    "    '''\n",
    "    error = []\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 0:\n",
    "            error.append(np.log(1-y_pr[i]))\n",
    "        if y[i] == 1:\n",
    "            error.append(np.log(y_pr[i]))\n",
    "    error = np.array(error).sum() / (-5)\n",
    "    \n",
    "        \n",
    "    return error + reg_const /(2*y.shape[0]) * (np.power(weights_3[:,1:], 2).sum() + np.power(weights_2[:,1:], 2).sum() + np.power(weights_1[:,1:], 2).sum())\n",
    "\n",
    "for num_epoch in range(epoch):\n",
    "    i = 0\n",
    "    for row in x_train:\n",
    "        pred = y_train[i, :]\n",
    "        pred = pred.reshape((5, 1))\n",
    "        \n",
    "        # forward propagation\n",
    "        a1, a2, a3, a4 = forward_prop(row, weights_1, weights_2, weights_3)\n",
    "        \n",
    "        # backward propagation\n",
    "        # Вот тут пиздец костыль конечно, но зато должно работать\n",
    "        # Всё для того чтобы застакать матрицу нужного размера\n",
    "        # const = (a4-pred) / ((1 - a4)*a4)\n",
    "        delta_3 = a4 - pred\n",
    "        if num_epoch == 0:\n",
    "            J_3 = delta_3\n",
    "            for each_element in a3:\n",
    "                vect_3 = delta_3 * each_element\n",
    "                J_3 = np.hstack((J_3, vect_3))\n",
    "            J_3 = np.delete(J_3, 0, 1)\n",
    "            J_3_no_reg = J_3\n",
    "            \n",
    "            # Adjusting regularization gradient\n",
    "            # Создаем копию матрицы весов (3), обнуляя первый столбец, чтобы смещение не регулязировалось\n",
    "            copy_of_weights_3 = np.insert(weights_3, 0, 0, axis=1)\n",
    "            copy_of_weights_3 = np.delete(copy_of_weights_3, 1, 1)\n",
    "            J_3 += reg_const * copy_of_weights_3\n",
    "            \n",
    "            \n",
    "            delta_2 = (a3 * (1 - a3))[1:, :]\n",
    "            J_2 = delta_2\n",
    "            for each_element in a2:\n",
    "                vect_2 = delta_2 * each_element\n",
    "                J_2 = np.hstack((J_2, vect_2))\n",
    "            J_2 = np.delete(J_2, 0, 1)\n",
    "            J_2 = J_3_no_reg.sum(axis=0).reshape(16, 1)[1:, :] * J_2\n",
    "            J_2_no_reg = J_2\n",
    "                \n",
    "            \n",
    "            copy_of_weights_2 = np.insert(weights_2, 0, 0, axis=1)\n",
    "            copy_of_weights_2 = np.delete(copy_of_weights_2, 1, 1)\n",
    "            J_2 += reg_const * copy_of_weights_2\n",
    "            \n",
    "            delta_1 = (a2 * (1 - a2))[1:, :]\n",
    "            J_1 = delta_1\n",
    "            for each_element in a1:\n",
    "                vect_1 = delta_1 * each_element\n",
    "                J_1 = np.hstack((J_1, vect_1))\n",
    "            J_1 = np.delete(J_1, 0, 1)\n",
    "            J_1 = J_2_no_reg.sum(axis=0).reshape(16, 1)[1:, :] * J_1\n",
    "            \n",
    "            copy_of_weights_1 = np.insert(weights_1, 0, 0, axis=1)\n",
    "            copy_of_weights_1 = np.delete(copy_of_weights_1, 1, 1)\n",
    "            J_1 += reg_const * copy_of_weights_1\n",
    "            \n",
    "           \n",
    "            \n",
    "        weights_3 -= alpha * J_3\n",
    "        weights_2 -= alpha * J_2\n",
    "        weights_1 -= alpha * J_1\n",
    "        \n",
    "        # error calculation\n",
    "        error = cross_entropy(pred, a4)\n",
    "        print(error)\n",
    "        \n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "answers = []\n",
    "for row in x_test:\n",
    "        pred = y_test[i, :]\n",
    "        pred = pred.reshape((5, 1))\n",
    "        \n",
    "        # forward propagation\n",
    "        a1, a2, a3, a4 = forward_prop(row, weights_1, weights_2, weights_3)\n",
    "        answers.append(a4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
