{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing all libraries required\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data labelled\n",
    "# !!! Note that the data has been previously cleaned in Excel, therefore there are no gaps and unforeseen signs !!!\n",
    "\n",
    "data = pd.read_csv('D:/123.csv', header=None)\n",
    "columnsNames = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'Pred']\n",
    "data.columns = columnsNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to the maximum value for each of the feature\n",
    "\n",
    "age = data.iloc[:, 0]\n",
    "age = np.array(age)\n",
    "age = age / 100\n",
    "data.iloc[:, 0] = age.astype(float)\n",
    "\n",
    "cp = data.iloc[:, 2]\n",
    "cp = np.array(cp)\n",
    "cp = cp / 4\n",
    "data.iloc[:, 2] = cp.astype(float)\n",
    "\n",
    "trestbps = data.iloc[:, 3]\n",
    "trestbps = np.array(trestbps)\n",
    "trestbps = trestbps / 200\n",
    "data.iloc[:, 3] = trestbps.astype(float)\n",
    "\n",
    "chol = data.iloc[:, 4]\n",
    "chol = np.array(chol)\n",
    "chol = chol / 564\n",
    "data.iloc[:, 4] = chol.astype(float)\n",
    "\n",
    "restecg = data.iloc[:, 6]\n",
    "restecg = np.array(restecg)\n",
    "restecg = restecg / 2\n",
    "data.iloc[:, 6] = restecg.astype(float)\n",
    "\n",
    "thalach = data.iloc[:, 7]\n",
    "thalach = np.array(thalach)\n",
    "thalach = thalach / 202\n",
    "data.iloc[:, 7] = thalach.astype(float)\n",
    "\n",
    "oldpeak = data.iloc[:, 9]\n",
    "oldpeak = np.array(oldpeak)\n",
    "oldpeak = oldpeak / 6.2\n",
    "data.iloc[:, 9] = oldpeak.astype(float)\n",
    "\n",
    "slope = data.iloc[:, 10]\n",
    "slope = np.array(slope)\n",
    "slope = slope / 3\n",
    "data.iloc[:, 10] = slope.astype(float)\n",
    "\n",
    "ca = data.iloc[:, 11]\n",
    "ca = np.array(ca)\n",
    "ca = ca / 3\n",
    "data.iloc[:, 11] = ca.astype(float)\n",
    "\n",
    "thal = data.iloc[:, 12]\n",
    "thal = np.array(thal)\n",
    "thal = thal / 7\n",
    "data.iloc[:, 12] = thal.astype(float)\n",
    "\n",
    "y_pred = data.iloc[:, -1]\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = y_pred / 4\n",
    "data.iloc[:, -1] = y_pred.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>297.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.545421</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.789562</td>\n",
       "      <td>0.658468</td>\n",
       "      <td>0.438564</td>\n",
       "      <td>0.144781</td>\n",
       "      <td>0.498316</td>\n",
       "      <td>0.740591</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.170251</td>\n",
       "      <td>0.534231</td>\n",
       "      <td>0.225589</td>\n",
       "      <td>0.675806</td>\n",
       "      <td>0.236532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.090497</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.241215</td>\n",
       "      <td>0.088814</td>\n",
       "      <td>0.092194</td>\n",
       "      <td>0.352474</td>\n",
       "      <td>0.497457</td>\n",
       "      <td>0.113572</td>\n",
       "      <td>0.469761</td>\n",
       "      <td>0.188084</td>\n",
       "      <td>0.206062</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>0.276947</td>\n",
       "      <td>0.308638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.374113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.658416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.430851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.757426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.610000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean     0.545421    0.676768    0.789562    0.658468    0.438564    0.144781   \n",
       "std      0.090497    0.468500    0.241215    0.088814    0.092194    0.352474   \n",
       "min      0.290000    0.000000    0.250000    0.470000    0.223404    0.000000   \n",
       "25%      0.480000    0.000000    0.750000    0.600000    0.374113    0.000000   \n",
       "50%      0.560000    1.000000    0.750000    0.650000    0.430851    0.000000   \n",
       "75%      0.610000    1.000000    1.000000    0.700000    0.489362    0.000000   \n",
       "max      0.770000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  297.000000  297.000000  297.000000  297.000000  297.000000  297.000000   \n",
       "mean     0.498316    0.740591    0.326599    0.170251    0.534231    0.225589   \n",
       "std      0.497457    0.113572    0.469761    0.188084    0.206062    0.312988   \n",
       "min      0.000000    0.351485    0.000000    0.000000    0.333333    0.000000   \n",
       "25%      0.000000    0.658416    0.000000    0.000000    0.333333    0.000000   \n",
       "50%      0.500000    0.757426    0.000000    0.129032    0.666667    0.000000   \n",
       "75%      1.000000    0.821782    1.000000    0.258065    0.666667    0.333333   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "             thal        Pred  \n",
       "count  297.000000  297.000000  \n",
       "mean     0.675806    0.236532  \n",
       "std      0.276947    0.308638  \n",
       "min      0.428571    0.000000  \n",
       "25%      0.428571    0.000000  \n",
       "50%      0.428571    0.000000  \n",
       "75%      1.000000    0.500000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47       1.         0.75       ... 0.         0.42857143 0.25      ]\n",
      " [0.59       1.         0.25       ... 0.         0.42857143 0.25      ]\n",
      " [0.46       0.         1.         ... 0.         0.42857143 0.        ]\n",
      " ...\n",
      " [0.47       1.         0.75       ... 0.         0.42857143 0.        ]\n",
      " [0.54       1.         1.         ... 0.33333333 1.         0.75      ]\n",
      " [0.51       1.         1.         ... 0.         1.         0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the data in order to avoid noт-random patterns in initial data\n",
    "\n",
    "matrix = data.iloc[:, :]\n",
    "matrix = np.array(matrix)\n",
    "np.random.shuffle(matrix)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing data by features and labels\n",
    "\n",
    "x_data = matrix[:, :13]\n",
    "y_data = matrix[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 5)\n",
      "(297, 13)\n"
     ]
    }
   ],
   "source": [
    "# Encoding labels in vectors\n",
    "\n",
    "def one_hot_encoding(y_data):\n",
    "    list = []\n",
    "    for i in range((y_data).shape[0]):\n",
    "        if y_data[i] == 0:\n",
    "            list.append([1, 0, 0, 0, 0,])\n",
    "        if y_data[i] == 0.25:\n",
    "            list.append([0, 1, 0, 0, 0,])\n",
    "        if y_data[i] == 0.5:\n",
    "            list.append([0, 0, 1, 0, 0,])\n",
    "        if y_data[i] == 0.75:\n",
    "            list.append([0, 0, 0, 1, 0,])\n",
    "        if y_data[i] == 1:\n",
    "            list.append([0, 0, 0, 0, 1,])\n",
    "    return(np.array(list))\n",
    "\n",
    "y_encoded = one_hot_encoding(y_data)\n",
    "print(y_encoded.shape)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 207 samples, validate on 90 samples\n",
      "Epoch 1/200\n",
      "207/207 [==============================] - 2s 8ms/step - loss: 1.1454 - acc: 0.4657 - val_loss: 1.1176 - val_acc: 0.5489\n",
      "Epoch 2/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 1.0967 - acc: 0.6039 - val_loss: 1.0770 - val_acc: 0.6911\n",
      "Epoch 3/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 1.0605 - acc: 0.7063 - val_loss: 1.0424 - val_acc: 0.7422\n",
      "Epoch 4/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 1.0283 - acc: 0.7643 - val_loss: 1.0096 - val_acc: 0.7867\n",
      "Epoch 5/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.9976 - acc: 0.7913 - val_loss: 0.9789 - val_acc: 0.7844\n",
      "Epoch 6/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.9686 - acc: 0.8029 - val_loss: 0.9493 - val_acc: 0.8000\n",
      "Epoch 7/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.9404 - acc: 0.8155 - val_loss: 0.9204 - val_acc: 0.8044\n",
      "Epoch 8/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.9130 - acc: 0.8193 - val_loss: 0.8926 - val_acc: 0.8044\n",
      "Epoch 9/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.8859 - acc: 0.8184 - val_loss: 0.8648 - val_acc: 0.8022\n",
      "Epoch 10/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.8591 - acc: 0.8126 - val_loss: 0.8377 - val_acc: 0.8022\n",
      "Epoch 11/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.8336 - acc: 0.8068 - val_loss: 0.8123 - val_acc: 0.8000\n",
      "Epoch 12/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.8096 - acc: 0.8048 - val_loss: 0.7888 - val_acc: 0.8000\n",
      "Epoch 13/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.7873 - acc: 0.8048 - val_loss: 0.7663 - val_acc: 0.8000\n",
      "Epoch 14/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.7662 - acc: 0.8048 - val_loss: 0.7452 - val_acc: 0.8000\n",
      "Epoch 15/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.7466 - acc: 0.8048 - val_loss: 0.7261 - val_acc: 0.8000\n",
      "Epoch 16/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.7286 - acc: 0.8039 - val_loss: 0.7086 - val_acc: 0.8000\n",
      "Epoch 17/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.7119 - acc: 0.8029 - val_loss: 0.6921 - val_acc: 0.8000\n",
      "Epoch 18/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.6963 - acc: 0.8019 - val_loss: 0.6761 - val_acc: 0.8000\n",
      "Epoch 19/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.6817 - acc: 0.8029 - val_loss: 0.6624 - val_acc: 0.8000\n",
      "Epoch 20/200\n",
      "207/207 [==============================] - 0s 217us/step - loss: 0.6685 - acc: 0.8029 - val_loss: 0.6493 - val_acc: 0.8000\n",
      "Epoch 21/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.6560 - acc: 0.8019 - val_loss: 0.6364 - val_acc: 0.8000\n",
      "Epoch 22/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.6440 - acc: 0.8029 - val_loss: 0.6247 - val_acc: 0.8000\n",
      "Epoch 23/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.6328 - acc: 0.8029 - val_loss: 0.6129 - val_acc: 0.8000\n",
      "Epoch 24/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.6220 - acc: 0.8039 - val_loss: 0.6019 - val_acc: 0.8000\n",
      "Epoch 25/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.6117 - acc: 0.8068 - val_loss: 0.5922 - val_acc: 0.8000\n",
      "Epoch 26/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.6022 - acc: 0.8116 - val_loss: 0.5828 - val_acc: 0.8022\n",
      "Epoch 27/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.5929 - acc: 0.8222 - val_loss: 0.5737 - val_acc: 0.8111\n",
      "Epoch 28/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.5842 - acc: 0.8184 - val_loss: 0.5650 - val_acc: 0.8178\n",
      "Epoch 29/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5756 - acc: 0.8261 - val_loss: 0.5568 - val_acc: 0.8222\n",
      "Epoch 30/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.5678 - acc: 0.8290 - val_loss: 0.5488 - val_acc: 0.8289\n",
      "Epoch 31/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.5600 - acc: 0.8357 - val_loss: 0.5414 - val_acc: 0.8289\n",
      "Epoch 32/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.5528 - acc: 0.8377 - val_loss: 0.5342 - val_acc: 0.8333\n",
      "Epoch 33/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.5457 - acc: 0.8435 - val_loss: 0.5273 - val_acc: 0.8356\n",
      "Epoch 34/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5391 - acc: 0.8435 - val_loss: 0.5210 - val_acc: 0.8356\n",
      "Epoch 35/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.5328 - acc: 0.8473 - val_loss: 0.5145 - val_acc: 0.8533\n",
      "Epoch 36/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.5267 - acc: 0.8551 - val_loss: 0.5085 - val_acc: 0.8711\n",
      "Epoch 37/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5207 - acc: 0.8560 - val_loss: 0.5026 - val_acc: 0.8733\n",
      "Epoch 38/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5151 - acc: 0.8676 - val_loss: 0.4977 - val_acc: 0.8733\n",
      "Epoch 39/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5098 - acc: 0.8638 - val_loss: 0.4925 - val_acc: 0.8733\n",
      "Epoch 40/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.5047 - acc: 0.8657 - val_loss: 0.4875 - val_acc: 0.8711\n",
      "Epoch 41/200\n",
      "207/207 [==============================] - 0s 231us/step - loss: 0.4999 - acc: 0.8647 - val_loss: 0.4830 - val_acc: 0.8733\n",
      "Epoch 42/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4955 - acc: 0.8628 - val_loss: 0.4780 - val_acc: 0.8711\n",
      "Epoch 43/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4909 - acc: 0.8657 - val_loss: 0.4739 - val_acc: 0.8711\n",
      "Epoch 44/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4870 - acc: 0.8628 - val_loss: 0.4706 - val_acc: 0.8733\n",
      "Epoch 45/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4831 - acc: 0.8686 - val_loss: 0.4665 - val_acc: 0.8733\n",
      "Epoch 46/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.4797 - acc: 0.8657 - val_loss: 0.4627 - val_acc: 0.8711\n",
      "Epoch 47/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4763 - acc: 0.8580 - val_loss: 0.4601 - val_acc: 0.8778\n",
      "Epoch 48/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4732 - acc: 0.8667 - val_loss: 0.4566 - val_acc: 0.8689\n",
      "Epoch 49/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.4700 - acc: 0.8628 - val_loss: 0.4538 - val_acc: 0.8667\n",
      "Epoch 50/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4671 - acc: 0.8647 - val_loss: 0.4512 - val_acc: 0.8711\n",
      "Epoch 51/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.4647 - acc: 0.8638 - val_loss: 0.4488 - val_acc: 0.8711\n",
      "Epoch 52/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4618 - acc: 0.8686 - val_loss: 0.4465 - val_acc: 0.8733\n",
      "Epoch 53/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4594 - acc: 0.8667 - val_loss: 0.4444 - val_acc: 0.8711\n",
      "Epoch 54/200\n",
      "207/207 [==============================] - 0s 226us/step - loss: 0.4574 - acc: 0.8628 - val_loss: 0.4419 - val_acc: 0.8644\n",
      "Epoch 55/200\n",
      "207/207 [==============================] - 0s 236us/step - loss: 0.4551 - acc: 0.8618 - val_loss: 0.4402 - val_acc: 0.8667\n",
      "Epoch 56/200\n",
      "207/207 [==============================] - 0s 222us/step - loss: 0.4532 - acc: 0.8657 - val_loss: 0.4387 - val_acc: 0.8733\n",
      "Epoch 57/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4516 - acc: 0.8638 - val_loss: 0.4370 - val_acc: 0.8733\n",
      "Epoch 58/200\n",
      "207/207 [==============================] - 0s 222us/step - loss: 0.4494 - acc: 0.8676 - val_loss: 0.4350 - val_acc: 0.8667\n",
      "Epoch 59/200\n",
      "207/207 [==============================] - 0s 212us/step - loss: 0.4480 - acc: 0.8638 - val_loss: 0.4341 - val_acc: 0.8756\n",
      "Epoch 60/200\n",
      "207/207 [==============================] - 0s 217us/step - loss: 0.4464 - acc: 0.8686 - val_loss: 0.4328 - val_acc: 0.8756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4451 - acc: 0.8667 - val_loss: 0.4318 - val_acc: 0.8778\n",
      "Epoch 62/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4436 - acc: 0.8696 - val_loss: 0.4297 - val_acc: 0.8667\n",
      "Epoch 63/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4423 - acc: 0.8676 - val_loss: 0.4281 - val_acc: 0.8644\n",
      "Epoch 64/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4410 - acc: 0.8696 - val_loss: 0.4272 - val_acc: 0.8711\n",
      "Epoch 65/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4396 - acc: 0.8705 - val_loss: 0.4259 - val_acc: 0.8667\n",
      "Epoch 66/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4385 - acc: 0.8657 - val_loss: 0.4252 - val_acc: 0.8711\n",
      "Epoch 67/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4377 - acc: 0.8696 - val_loss: 0.4236 - val_acc: 0.8622\n",
      "Epoch 68/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4364 - acc: 0.8657 - val_loss: 0.4229 - val_acc: 0.8622\n",
      "Epoch 69/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4356 - acc: 0.8657 - val_loss: 0.4221 - val_acc: 0.8622\n",
      "Epoch 70/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4348 - acc: 0.8628 - val_loss: 0.4216 - val_acc: 0.8600\n",
      "Epoch 71/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4338 - acc: 0.8647 - val_loss: 0.4206 - val_acc: 0.8622\n",
      "Epoch 72/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4327 - acc: 0.8686 - val_loss: 0.4195 - val_acc: 0.8622\n",
      "Epoch 73/200\n",
      "207/207 [==============================] - 0s 168us/step - loss: 0.4318 - acc: 0.8638 - val_loss: 0.4190 - val_acc: 0.8644\n",
      "Epoch 74/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4314 - acc: 0.8638 - val_loss: 0.4180 - val_acc: 0.8622\n",
      "Epoch 75/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4302 - acc: 0.8647 - val_loss: 0.4176 - val_acc: 0.8644\n",
      "Epoch 76/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4295 - acc: 0.8657 - val_loss: 0.4171 - val_acc: 0.8600\n",
      "Epoch 77/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4286 - acc: 0.8647 - val_loss: 0.4169 - val_acc: 0.8644\n",
      "Epoch 78/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4277 - acc: 0.8667 - val_loss: 0.4167 - val_acc: 0.8711\n",
      "Epoch 79/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4268 - acc: 0.8676 - val_loss: 0.4154 - val_acc: 0.8644\n",
      "Epoch 80/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4258 - acc: 0.8696 - val_loss: 0.4146 - val_acc: 0.8644\n",
      "Epoch 81/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4251 - acc: 0.8676 - val_loss: 0.4141 - val_acc: 0.8667\n",
      "Epoch 82/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4243 - acc: 0.8686 - val_loss: 0.4136 - val_acc: 0.8667\n",
      "Epoch 83/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4232 - acc: 0.8696 - val_loss: 0.4123 - val_acc: 0.8644\n",
      "Epoch 84/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4224 - acc: 0.8705 - val_loss: 0.4122 - val_acc: 0.8667\n",
      "Epoch 85/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.4217 - acc: 0.8686 - val_loss: 0.4109 - val_acc: 0.8622\n",
      "Epoch 86/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4208 - acc: 0.8657 - val_loss: 0.4113 - val_acc: 0.8778\n",
      "Epoch 87/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4200 - acc: 0.8686 - val_loss: 0.4104 - val_acc: 0.8756\n",
      "Epoch 88/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4193 - acc: 0.8686 - val_loss: 0.4100 - val_acc: 0.8756\n",
      "Epoch 89/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.4186 - acc: 0.8676 - val_loss: 0.4090 - val_acc: 0.8733\n",
      "Epoch 90/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4178 - acc: 0.8686 - val_loss: 0.4077 - val_acc: 0.8644\n",
      "Epoch 91/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4169 - acc: 0.8676 - val_loss: 0.4075 - val_acc: 0.8667\n",
      "Epoch 92/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4165 - acc: 0.8686 - val_loss: 0.4072 - val_acc: 0.8689\n",
      "Epoch 93/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4155 - acc: 0.8696 - val_loss: 0.4063 - val_acc: 0.8667\n",
      "Epoch 94/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4147 - acc: 0.8676 - val_loss: 0.4053 - val_acc: 0.8644\n",
      "Epoch 95/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4139 - acc: 0.8686 - val_loss: 0.4047 - val_acc: 0.8644\n",
      "Epoch 96/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.4134 - acc: 0.8686 - val_loss: 0.4055 - val_acc: 0.8756\n",
      "Epoch 97/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4125 - acc: 0.8686 - val_loss: 0.4048 - val_acc: 0.8756\n",
      "Epoch 98/200\n",
      "207/207 [==============================] - 0s 212us/step - loss: 0.4122 - acc: 0.8705 - val_loss: 0.4043 - val_acc: 0.8756\n",
      "Epoch 99/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4111 - acc: 0.8676 - val_loss: 0.4031 - val_acc: 0.8689\n",
      "Epoch 100/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4105 - acc: 0.8696 - val_loss: 0.4035 - val_acc: 0.8733\n",
      "Epoch 101/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4097 - acc: 0.8686 - val_loss: 0.4026 - val_acc: 0.8756\n",
      "Epoch 102/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.4090 - acc: 0.8676 - val_loss: 0.4018 - val_acc: 0.8756\n",
      "Epoch 103/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4082 - acc: 0.8686 - val_loss: 0.4011 - val_acc: 0.8711\n",
      "Epoch 104/200\n",
      "207/207 [==============================] - 0s 202us/step - loss: 0.4078 - acc: 0.8686 - val_loss: 0.4002 - val_acc: 0.8733\n",
      "Epoch 105/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4069 - acc: 0.8676 - val_loss: 0.4006 - val_acc: 0.8756\n",
      "Epoch 106/200\n",
      "207/207 [==============================] - 0s 159us/step - loss: 0.4060 - acc: 0.8696 - val_loss: 0.3992 - val_acc: 0.8733\n",
      "Epoch 107/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4053 - acc: 0.8676 - val_loss: 0.3992 - val_acc: 0.8756\n",
      "Epoch 108/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.4047 - acc: 0.8686 - val_loss: 0.3987 - val_acc: 0.8756\n",
      "Epoch 109/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4041 - acc: 0.8696 - val_loss: 0.3993 - val_acc: 0.8733\n",
      "Epoch 110/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4037 - acc: 0.8705 - val_loss: 0.3995 - val_acc: 0.8733\n",
      "Epoch 111/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.4034 - acc: 0.8676 - val_loss: 0.3976 - val_acc: 0.8756\n",
      "Epoch 112/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4024 - acc: 0.8686 - val_loss: 0.3974 - val_acc: 0.8756\n",
      "Epoch 113/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.4019 - acc: 0.8676 - val_loss: 0.3964 - val_acc: 0.8756\n",
      "Epoch 114/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.4010 - acc: 0.8696 - val_loss: 0.3964 - val_acc: 0.8756\n",
      "Epoch 115/200\n",
      "207/207 [==============================] - 0s 226us/step - loss: 0.4008 - acc: 0.8686 - val_loss: 0.3953 - val_acc: 0.8756\n",
      "Epoch 116/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.4002 - acc: 0.8676 - val_loss: 0.3952 - val_acc: 0.8756\n",
      "Epoch 117/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.3994 - acc: 0.8686 - val_loss: 0.3948 - val_acc: 0.8756\n",
      "Epoch 118/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3990 - acc: 0.8686 - val_loss: 0.3951 - val_acc: 0.8756\n",
      "Epoch 119/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3985 - acc: 0.8696 - val_loss: 0.3937 - val_acc: 0.8756\n",
      "Epoch 120/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3979 - acc: 0.8686 - val_loss: 0.3929 - val_acc: 0.8711\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 0s 173us/step - loss: 0.3977 - acc: 0.8696 - val_loss: 0.3933 - val_acc: 0.8756\n",
      "Epoch 122/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3967 - acc: 0.8696 - val_loss: 0.3938 - val_acc: 0.8733\n",
      "Epoch 123/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3962 - acc: 0.8696 - val_loss: 0.3942 - val_acc: 0.8778\n",
      "Epoch 124/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3959 - acc: 0.8676 - val_loss: 0.3918 - val_acc: 0.8756\n",
      "Epoch 125/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3950 - acc: 0.8686 - val_loss: 0.3916 - val_acc: 0.8756\n",
      "Epoch 126/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.3950 - acc: 0.8686 - val_loss: 0.3908 - val_acc: 0.8756\n",
      "Epoch 127/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3943 - acc: 0.8676 - val_loss: 0.3905 - val_acc: 0.8756\n",
      "Epoch 128/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.3938 - acc: 0.8686 - val_loss: 0.3903 - val_acc: 0.8756\n",
      "Epoch 129/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3933 - acc: 0.8676 - val_loss: 0.3895 - val_acc: 0.8733\n",
      "Epoch 130/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3929 - acc: 0.8696 - val_loss: 0.3897 - val_acc: 0.8756\n",
      "Epoch 131/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3924 - acc: 0.8696 - val_loss: 0.3903 - val_acc: 0.8733\n",
      "Epoch 132/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3920 - acc: 0.8686 - val_loss: 0.3886 - val_acc: 0.8756\n",
      "Epoch 133/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3915 - acc: 0.8686 - val_loss: 0.3883 - val_acc: 0.8756\n",
      "Epoch 134/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3915 - acc: 0.8705 - val_loss: 0.3885 - val_acc: 0.8756\n",
      "Epoch 135/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3906 - acc: 0.8696 - val_loss: 0.3897 - val_acc: 0.8778\n",
      "Epoch 136/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3901 - acc: 0.8696 - val_loss: 0.3878 - val_acc: 0.8756\n",
      "Epoch 137/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3897 - acc: 0.8715 - val_loss: 0.3869 - val_acc: 0.8733\n",
      "Epoch 138/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3899 - acc: 0.8696 - val_loss: 0.3868 - val_acc: 0.8756\n",
      "Epoch 139/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3889 - acc: 0.8715 - val_loss: 0.3877 - val_acc: 0.8778\n",
      "Epoch 140/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3888 - acc: 0.8696 - val_loss: 0.3872 - val_acc: 0.8778\n",
      "Epoch 141/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3879 - acc: 0.8715 - val_loss: 0.3860 - val_acc: 0.8756\n",
      "Epoch 142/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3878 - acc: 0.8696 - val_loss: 0.3861 - val_acc: 0.8733\n",
      "Epoch 143/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3876 - acc: 0.8705 - val_loss: 0.3852 - val_acc: 0.8756\n",
      "Epoch 144/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3874 - acc: 0.8705 - val_loss: 0.3847 - val_acc: 0.8733\n",
      "Epoch 145/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3865 - acc: 0.8705 - val_loss: 0.3859 - val_acc: 0.8800\n",
      "Epoch 146/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3864 - acc: 0.8696 - val_loss: 0.3852 - val_acc: 0.8800\n",
      "Epoch 147/200\n",
      "207/207 [==============================] - 0s 212us/step - loss: 0.3857 - acc: 0.8705 - val_loss: 0.3843 - val_acc: 0.8733\n",
      "Epoch 148/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3854 - acc: 0.8715 - val_loss: 0.3840 - val_acc: 0.8733\n",
      "Epoch 149/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.3848 - acc: 0.8715 - val_loss: 0.3861 - val_acc: 0.8800\n",
      "Epoch 150/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3849 - acc: 0.8696 - val_loss: 0.3839 - val_acc: 0.8800\n",
      "Epoch 151/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3843 - acc: 0.8705 - val_loss: 0.3845 - val_acc: 0.8822\n",
      "Epoch 152/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3838 - acc: 0.8705 - val_loss: 0.3840 - val_acc: 0.8822\n",
      "Epoch 153/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3836 - acc: 0.8676 - val_loss: 0.3830 - val_acc: 0.8800\n",
      "Epoch 154/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3829 - acc: 0.8715 - val_loss: 0.3844 - val_acc: 0.8800\n",
      "Epoch 155/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3833 - acc: 0.8715 - val_loss: 0.3829 - val_acc: 0.8822\n",
      "Epoch 156/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3821 - acc: 0.8696 - val_loss: 0.3814 - val_acc: 0.8733\n",
      "Epoch 157/200\n",
      "207/207 [==============================] - 0s 212us/step - loss: 0.3819 - acc: 0.8715 - val_loss: 0.3818 - val_acc: 0.8800\n",
      "Epoch 158/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3817 - acc: 0.8705 - val_loss: 0.3807 - val_acc: 0.8711\n",
      "Epoch 159/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3819 - acc: 0.8686 - val_loss: 0.3810 - val_acc: 0.8778\n",
      "Epoch 160/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3814 - acc: 0.8705 - val_loss: 0.3812 - val_acc: 0.8822\n",
      "Epoch 161/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.3806 - acc: 0.8725 - val_loss: 0.3826 - val_acc: 0.8800\n",
      "Epoch 162/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3806 - acc: 0.8696 - val_loss: 0.3814 - val_acc: 0.8822\n",
      "Epoch 163/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3799 - acc: 0.8705 - val_loss: 0.3798 - val_acc: 0.8778\n",
      "Epoch 164/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3802 - acc: 0.8705 - val_loss: 0.3795 - val_acc: 0.8778\n",
      "Epoch 165/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3798 - acc: 0.8686 - val_loss: 0.3786 - val_acc: 0.8711\n",
      "Epoch 166/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3791 - acc: 0.8715 - val_loss: 0.3798 - val_acc: 0.8822\n",
      "Epoch 167/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.3790 - acc: 0.8754 - val_loss: 0.3796 - val_acc: 0.8822\n",
      "Epoch 168/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3786 - acc: 0.8705 - val_loss: 0.3788 - val_acc: 0.8800\n",
      "Epoch 169/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3782 - acc: 0.8725 - val_loss: 0.3799 - val_acc: 0.8822\n",
      "Epoch 170/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3781 - acc: 0.8696 - val_loss: 0.3790 - val_acc: 0.8822\n",
      "Epoch 171/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3778 - acc: 0.8715 - val_loss: 0.3805 - val_acc: 0.8822\n",
      "Epoch 172/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3773 - acc: 0.8686 - val_loss: 0.3779 - val_acc: 0.8800\n",
      "Epoch 173/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3774 - acc: 0.8715 - val_loss: 0.3793 - val_acc: 0.8822\n",
      "Epoch 174/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3770 - acc: 0.8686 - val_loss: 0.3787 - val_acc: 0.8822\n",
      "Epoch 175/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3767 - acc: 0.8725 - val_loss: 0.3782 - val_acc: 0.8822\n",
      "Epoch 176/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3764 - acc: 0.8705 - val_loss: 0.3770 - val_acc: 0.8800\n",
      "Epoch 177/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3764 - acc: 0.8705 - val_loss: 0.3776 - val_acc: 0.8822\n",
      "Epoch 178/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3758 - acc: 0.8705 - val_loss: 0.3761 - val_acc: 0.8711\n",
      "Epoch 179/200\n",
      "207/207 [==============================] - 0s 217us/step - loss: 0.3757 - acc: 0.8676 - val_loss: 0.3762 - val_acc: 0.8800\n",
      "Epoch 180/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3753 - acc: 0.8705 - val_loss: 0.3761 - val_acc: 0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.3751 - acc: 0.8725 - val_loss: 0.3760 - val_acc: 0.8800\n",
      "Epoch 182/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3749 - acc: 0.8725 - val_loss: 0.3766 - val_acc: 0.8822\n",
      "Epoch 183/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.3747 - acc: 0.8696 - val_loss: 0.3766 - val_acc: 0.8822\n",
      "Epoch 184/200\n",
      "207/207 [==============================] - 0s 193us/step - loss: 0.3740 - acc: 0.8734 - val_loss: 0.3764 - val_acc: 0.8822\n",
      "Epoch 185/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3741 - acc: 0.8705 - val_loss: 0.3747 - val_acc: 0.8711\n",
      "Epoch 186/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.3738 - acc: 0.8715 - val_loss: 0.3752 - val_acc: 0.8822\n",
      "Epoch 187/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.3740 - acc: 0.8705 - val_loss: 0.3749 - val_acc: 0.8822\n",
      "Epoch 188/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3737 - acc: 0.8734 - val_loss: 0.3747 - val_acc: 0.8822\n",
      "Epoch 189/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3730 - acc: 0.8715 - val_loss: 0.3743 - val_acc: 0.8800\n",
      "Epoch 190/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3730 - acc: 0.8725 - val_loss: 0.3743 - val_acc: 0.8822\n",
      "Epoch 191/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3729 - acc: 0.8705 - val_loss: 0.3742 - val_acc: 0.8822\n",
      "Epoch 192/200\n",
      "207/207 [==============================] - 0s 207us/step - loss: 0.3724 - acc: 0.8715 - val_loss: 0.3737 - val_acc: 0.8778\n",
      "Epoch 193/200\n",
      "207/207 [==============================] - 0s 173us/step - loss: 0.3720 - acc: 0.8725 - val_loss: 0.3747 - val_acc: 0.8822\n",
      "Epoch 194/200\n",
      "207/207 [==============================] - 0s 198us/step - loss: 0.3720 - acc: 0.8715 - val_loss: 0.3741 - val_acc: 0.8822\n",
      "Epoch 195/200\n",
      "207/207 [==============================] - 0s 169us/step - loss: 0.3716 - acc: 0.8725 - val_loss: 0.3732 - val_acc: 0.8800\n",
      "Epoch 196/200\n",
      "207/207 [==============================] - 0s 164us/step - loss: 0.3714 - acc: 0.8725 - val_loss: 0.3753 - val_acc: 0.8822\n",
      "Epoch 197/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3719 - acc: 0.8705 - val_loss: 0.3743 - val_acc: 0.8822\n",
      "Epoch 198/200\n",
      "207/207 [==============================] - 0s 188us/step - loss: 0.3714 - acc: 0.8705 - val_loss: 0.3725 - val_acc: 0.8800\n",
      "Epoch 199/200\n",
      "207/207 [==============================] - 0s 183us/step - loss: 0.3708 - acc: 0.8744 - val_loss: 0.3744 - val_acc: 0.8822\n",
      "Epoch 200/200\n",
      "207/207 [==============================] - 0s 178us/step - loss: 0.3707 - acc: 0.8725 - val_loss: 0.3728 - val_acc: 0.8822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x220b507cf60>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building model within \"One-vs-All\" algorithm.\n",
    "# For better understanding check this out:\n",
    "# https://www.coursera.org/learn/machine-learning/lecture/gFpiW/multiclass-classification\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(11, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dense(11, activation=tf.nn.relu, kernel_regularizer=regularizers.l2(0.02)),\n",
    "    tf.keras.layers.Dense(5, activation=tf.nn.sigmoid)\n",
    "])\n",
    "model.compile(optimizer='RMSProp',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_data, y_encoded, validation_split=0.3, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total training accuracy: 0.8725\n",
    "Total validation accuracy: 0.8822"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
